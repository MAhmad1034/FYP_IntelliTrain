{
 "cells": [

  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a130506-e799-4af8-a4ff-33b712c819c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eaf6e9d-4d33-4297-ac69-3418033b41f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5694e00-1a0e-41e4-ade8-c79426cdf188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to datasets\n",
    "data_dir = \"C:/Users/5A_Traders/Desktop/GTM Data\"\n",
    "image_size = (224, 224)  # MobileNetV2 input size\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "497fc58a-8845-4669-940c-7667048a4bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    validation_split=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.3,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1406626d-20dc-49e8-ae67-45d06a989682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1203 images belonging to 3 classes.\n",
      "Found 300 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    subset=\"training\",\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    subset=\"validation\",\n",
    "    class_mode=\"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc27da2c-0096-4bdc-a810-748808c51082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1203,)\n"
     ]
    }
   ],
   "source": [
    "print(train_generator.labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d7d1250-b540-4f6c-8feb-4606194dd2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1.0100755667506298, 1: 0.9901234567901235, 2: 1.0}\n"
     ]
    }
   ],
   "source": [
    "class_indices = train_generator.class_indices\n",
    "class_labels = list(class_indices.keys())\n",
    "\n",
    "# Compute class weights to handle class imbalance\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.arange(len(class_labels)),  # Class labels as integers\n",
    "    y=train_generator.labels  # Use labels directly\n",
    ")\n",
    "\n",
    "# Convert to a dictionary to map class indices to weights\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "print(class_weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98eae272-ccfb-4ada-90ff-287e33e8bb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Fine-tuning: Unfreeze the last few layers of the base model\n",
    "for layer in base_model.layers[-30:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c350d38d-e34c-4958-80eb-c4e7f0cca720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom layers on top\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dropout(0.3)(x)\n",
    "predictions = Dense(train_generator.num_classes, activation=\"softmax\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1a73c2a-df06-4aeb-9d04-78a7dbc119d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=base_model.input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e82cd42-5055-4361-a2d8-46a4fee4308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with gradient clipping\n",
    "opt = Adam(learning_rate=learning_rate, clipnorm=1.0)\n",
    "model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd5d6b28-b1de-45ec-9fec-11bb0f668a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping and model checkpoint\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model.keras\", monitor=\"val_loss\", save_best_only=True\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69c6aa49-7d2d-4198-b9b6-9fb0b2c04e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\5A_Traders\\miniconda3\\envs\\GTM\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 4s/step - accuracy: 0.6088 - loss: 0.8507 - val_accuracy: 0.9688 - val_loss: 0.1135\n",
      "Epoch 2/10\n",
      "\u001b[1m 1/37\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:05\u001b[0m 3s/step - accuracy: 0.8438 - loss: 0.2595"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\5A_Traders\\miniconda3\\envs\\GTM\\lib\\contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - accuracy: 0.8438 - loss: 0.2595 - val_accuracy: 1.0000 - val_loss: 0.0065\n",
      "Epoch 3/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 4s/step - accuracy: 0.9494 - loss: 0.1466 - val_accuracy: 0.9653 - val_loss: 0.0800\n",
      "Epoch 4/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9688 - loss: 0.0789 - val_accuracy: 0.9167 - val_loss: 0.2928\n",
      "Epoch 5/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 4s/step - accuracy: 0.9712 - loss: 0.0778 - val_accuracy: 0.9722 - val_loss: 0.0639\n",
      "Epoch 6/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9688 - loss: 0.0581 - val_accuracy: 1.0000 - val_loss: 0.0171\n",
      "Epoch 7/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 4s/step - accuracy: 0.9778 - loss: 0.0677 - val_accuracy: 0.9653 - val_loss: 0.1163\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_steps=val_generator.samples // batch_size,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97cf1f21-25e2-4c7a-adac-98a54a172997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the final model\n",
    "model.save(\"mobilenetv2_few_shot_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4cbb09d-4025-42be-9a32-d85a0ff0aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"mobilenetv2_image_classifier.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2056321f-ec01-45c4-a994-45e9a2662575",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prediction Function for Few-Shot Learning\n",
    "def predict_image(image_path):\n",
    "    from tensorflow.keras.preprocessing import image\n",
    "    img = image.load_img(image_path, target_size=image_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = np.argmax(predictions)\n",
    "    confidence = np.max(predictions)\n",
    "    return train_generator.class_indices, predicted_class, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fda281cd-4a17-40ec-9a97-3fce3479cbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "Class Indices: {'cats': 0, 'dogs': 1, 'mountain': 2}\n",
      "Predicted Class: 2\n",
      "Confidence: 0.8341597\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "class_indices, predicted_class, confidence = predict_image(\"C:/Users/5A_Traders/Desktop/TestData/pexels-suju-1271816.jpg\")\n",
    "print(\"Class Indices:\", class_indices)\n",
    "print(\"Predicted Class:\", predicted_class)\n",
    "print(\"Confidence:\", confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0bdbc2-82c2-4bc3-9266-9d1717fe4a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
