{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1063507c-1054-4971-804e-0a55a8d2513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "253d49ac-cb04-4198-8c02-e8b2ecbf3123",
   "metadata": {},
   "outputs": [],
   "source": [
    "yamnet_model_handle = \"https://tfhub.dev/google/yamnet/1\"\n",
    "yamnet = hub.load(yamnet_model_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e3d8d8bc-f290-4dfb-ae3c-4a9529a5c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embedding(audio_file):\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_file, sr=16000, mono=True)\n",
    "        waveform = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "        scores, embeddings, spectrogram = yamnet(waveform)\n",
    "        \n",
    "        # Ensure embeddings are 3D (batch, time, features)\n",
    "        if len(embeddings.shape) == 2:\n",
    "            embeddings = tf.expand_dims(embeddings, axis=0)  # Add batch dim\n",
    "        \n",
    "        # Compute mean over time axis (axis=1)\n",
    "        mean_embedding = np.mean(embeddings.numpy()[0], axis=0)\n",
    "        return mean_embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_file}: {e}\")\n",
    "        return np.zeros(1024)  # Return a zero vector if extraction fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e5e11cb2-cad2-4f2a-b3ac-9867c022357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "audio_dir = \"\"\n",
    "# Load dataset\n",
    "train_audio_dir = \"C:/Users/5A_Traders/Downloads/FYP_ON_DEV/FYP_IntelliTrain/AudioClassification/Dataset/archive/DataTrain\"\n",
    "test_audio_dir = \"C:/Users/5A_Traders/Downloads/FYP_ON_DEV/FYP_IntelliTrain/AudioClassification/Dataset/archive/DataTest\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c80c4832-9c72-4b2a-8f9a-ee5d721906e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory):\n",
    "    labels, features = [], []\n",
    "    for class_label in os.listdir(directory):\n",
    "        class_path = os.path.join(directory, class_label)\n",
    "        if os.path.isdir(class_path):\n",
    "            for audio_file in os.listdir(class_path):\n",
    "                file_path = os.path.join(class_path, audio_file)\n",
    "                try:\n",
    "                    feature = extract_embedding(file_path)\n",
    "                    features.append(feature)\n",
    "                    labels.append(class_label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "    return np.array(features), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cbf75e5d-85e4-4006-85a3-cb1964becc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (10, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Load ALL data (train + test) or adjust paths if you want separate train/test dirs\n",
    "# Here we assume you're using DataTrain for training and DataTest for testing\n",
    "X_train, y_train = load_data(train_audio_dir)\n",
    "X_test, y_test = load_data(test_audio_dir)\n",
    "\n",
    "# Encode labels using LabelEncoder (fit on ALL labels to cover all classes)\n",
    "le = LabelEncoder()\n",
    "le.fit(np.concatenate([y_train, y_test]))  # Fit on all possible labels\n",
    "\n",
    "# Transform string labels to integers\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "# Convert features to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "\n",
    "# If features are 1D (due to errors), reshape to (n_samples, 1024)\n",
    "if X_train.ndim == 1:\n",
    "    X_train = X_train.reshape(-1, 1024)\n",
    "    X_test = X_test.reshape(-1, 1024)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)  # Should be (n_samples, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2bb25dfc-a383-406a-8fa1-6ef743d89bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (10, 1024)\n",
      "Example feature shape: (1024,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)  # Should be (n_samples, 1024)\n",
    "print(\"Example feature shape:\", X_train[0].shape)  # Should be (1024,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "74bac17b-ae52-43c4-b6c0-9ac8c9c013e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Model\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),  # Shape (1024,)\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(le.classes_), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4e90a95d-521a-445d-ad16-dd44b21e483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',  # For integer-encoded labels\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7deade4e-1cf6-43e7-a7fb-c50b3788a301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.0000e+00 - loss: 2.1761 - val_accuracy: 0.3077 - val_loss: 1.5369\n",
      "Epoch 2/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.3000 - loss: 1.6143 - val_accuracy: 0.5385 - val_loss: 1.2988\n",
      "Epoch 3/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.7000 - loss: 1.1691 - val_accuracy: 0.7692 - val_loss: 1.2058\n",
      "Epoch 4/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8000 - loss: 0.9770 - val_accuracy: 0.7692 - val_loss: 1.0701\n",
      "Epoch 5/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.9000 - loss: 0.7414 - val_accuracy: 0.6923 - val_loss: 1.0446\n",
      "Epoch 6/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8000 - loss: 0.6795 - val_accuracy: 0.6154 - val_loss: 1.0347\n",
      "Epoch 7/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 1.0000 - loss: 0.5611 - val_accuracy: 0.6154 - val_loss: 0.9671\n",
      "Epoch 8/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.9000 - loss: 0.5636 - val_accuracy: 0.6154 - val_loss: 0.8551\n",
      "Epoch 9/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.9000 - loss: 0.3771 - val_accuracy: 0.6923 - val_loss: 0.8000\n",
      "Epoch 10/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9000 - loss: 0.5101 - val_accuracy: 0.6154 - val_loss: 0.8047\n",
      "Epoch 11/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9000 - loss: 0.6106 - val_accuracy: 0.6154 - val_loss: 0.9019\n",
      "Epoch 12/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 1.0000 - loss: 0.1352 - val_accuracy: 0.6154 - val_loss: 1.0064\n",
      "Epoch 13/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 1.0000 - loss: 0.1327 - val_accuracy: 0.6154 - val_loss: 1.1408\n",
      "Epoch 14/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.9000 - loss: 0.3763 - val_accuracy: 0.6154 - val_loss: 1.1830\n",
      "Epoch 15/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.9000 - loss: 0.3029 - val_accuracy: 0.6154 - val_loss: 1.1312\n",
      "Epoch 16/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 1.0000 - loss: 0.0877 - val_accuracy: 0.6154 - val_loss: 1.0566\n",
      "Epoch 17/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9000 - loss: 0.3549 - val_accuracy: 0.6154 - val_loss: 0.9578\n",
      "Epoch 18/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.9000 - loss: 0.2994 - val_accuracy: 0.6154 - val_loss: 0.8444\n",
      "Epoch 19/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.7000 - loss: 0.4857 - val_accuracy: 0.6154 - val_loss: 0.8153\n",
      "Epoch 20/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9000 - loss: 0.2215 - val_accuracy: 0.6154 - val_loss: 0.8232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2cd3aa41ed0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df831ec-d6b1-464c-a214-5809d54a87a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "model.save(\"audio_classification_model.h5\")\n",
    "print(\"Model training complete and saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
