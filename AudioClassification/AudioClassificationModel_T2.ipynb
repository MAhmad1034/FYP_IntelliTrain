{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1063507c-1054-4971-804e-0a55a8d2513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "253d49ac-cb04-4198-8c02-e8b2ecbf3123",
   "metadata": {},
   "outputs": [],
   "source": [
    "yamnet_model_handle = \"https://tfhub.dev/google/yamnet/1\"\n",
    "yamnet = hub.load(yamnet_model_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e3d8d8bc-f290-4dfb-ae3c-4a9529a5c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embedding(audio_file):\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_file, sr=16000, mono=True)\n",
    "        waveform = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "        scores, embeddings, spectrogram = yamnet(waveform)\n",
    "        \n",
    "        # Ensure embeddings are 3D (batch, time, features)\n",
    "        if len(embeddings.shape) == 2:\n",
    "            embeddings = tf.expand_dims(embeddings, axis=0)  # Add batch dim\n",
    "        \n",
    "        # Compute mean over time axis (axis=1)\n",
    "        mean_embedding = np.mean(embeddings.numpy()[0], axis=0)\n",
    "        return mean_embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_file}: {e}\")\n",
    "        return np.zeros(1024)  # Return a zero vector if extraction fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e5e11cb2-cad2-4f2a-b3ac-9867c022357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "audio_dir = \"\"\n",
    "# Load dataset\n",
    "audio_dir = \"C:/Users/5A_Traders/Downloads/FYP_ON_DEV/FYP_IntelliTrain/AudioClassification/Dataset/animal_audio/Animal-Soundprepros\"\n",
    "# test_audio_dir = \"C:/Users/5A_Traders/Downloads/FYP_ON_DEV/FYP_IntelliTrain/AudioClassification/Dataset/archive/DataTest\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c80c4832-9c72-4b2a-8f9a-ee5d721906e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory):\n",
    "    labels, features = [], []\n",
    "    for class_label in os.listdir(directory):\n",
    "        class_path = os.path.join(directory, class_label)\n",
    "        if os.path.isdir(class_path):\n",
    "            for audio_file in os.listdir(class_path):\n",
    "                file_path = os.path.join(class_path, audio_file)\n",
    "                try:\n",
    "                    feature = extract_embedding(file_path)\n",
    "                    features.append(feature)\n",
    "                    labels.append(class_label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "    return np.array(features), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cbf75e5d-85e4-4006-85a3-cb1964becc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ALL data (train + test) or adjust paths if you want separate train/test dirs\n",
    "# Here we assume you're using DataTrain for training and DataTest for testing\n",
    "X, y = load_data(audio_dir)\n",
    "# X_test, y_test = load_data(test_audio_dir)\n",
    "\n",
    "# Encode labels using LabelEncoder (fit on ALL labels to cover all classes)\n",
    "# le = LabelEncoder()\n",
    "# le.fit(np.concatenate([y_train, y_test]))  # Fit on all possible labels\n",
    "\n",
    "# # Transform string labels to integers\n",
    "# y_train = le.transform(y_train)\n",
    "# y_test = le.transform(y_test)\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "labels_encoded = le.transform(y)\n",
    "X = np.array(X)\n",
    "y = np.array(labels_encoded)\n",
    "\n",
    "# Convert features to numpy arrays\n",
    "# X_train = np.array(X_train)\n",
    "# X_test = np.array(X_test)\n",
    "\n",
    "\n",
    "# # If features are 1D (due to errors), reshape to (n_samples, 1024)\n",
    "# if X_train.ndim == 1:\n",
    "#     X_train = X_train.reshape(-1, 1024)\n",
    "#     X_test = X_test.reshape(-1, 1024)\n",
    "\n",
    "# print(\"X_train shape:\", X_train.shape)  # Should be (n_samples, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f125695f-2a0b-4bbe-96c8-4b54ceef3d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2bb25dfc-a383-406a-8fa1-6ef743d89bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (520, 1024)\n",
      "Example feature shape: (1024,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)  # Should be (n_samples, 1024)\n",
    "print(\"Example feature shape:\", X_train[0].shape)  # Should be (1024,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "74bac17b-ae52-43c4-b6c0-9ac8c9c013e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Model\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),  # Shape (1024,)\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(le.classes_), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4e90a95d-521a-445d-ad16-dd44b21e483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',  # For integer-encoded labels\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7deade4e-1cf6-43e7-a7fb-c50b3788a301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.2443 - loss: 2.2816 - val_accuracy: 0.6692 - val_loss: 1.2910\n",
      "Epoch 2/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6586 - loss: 1.2447 - val_accuracy: 0.7538 - val_loss: 0.8284\n",
      "Epoch 3/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7012 - loss: 0.8989 - val_accuracy: 0.7462 - val_loss: 0.8225\n",
      "Epoch 4/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7997 - loss: 0.6907 - val_accuracy: 0.7846 - val_loss: 0.6753\n",
      "Epoch 5/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8579 - loss: 0.5543 - val_accuracy: 0.7846 - val_loss: 0.6733\n",
      "Epoch 6/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8439 - loss: 0.5052 - val_accuracy: 0.8154 - val_loss: 0.6543\n",
      "Epoch 7/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8906 - loss: 0.3938 - val_accuracy: 0.7923 - val_loss: 0.7091\n",
      "Epoch 8/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8978 - loss: 0.3368 - val_accuracy: 0.8231 - val_loss: 0.6080\n",
      "Epoch 9/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8989 - loss: 0.3272 - val_accuracy: 0.8154 - val_loss: 0.6215\n",
      "Epoch 10/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9555 - loss: 0.2407 - val_accuracy: 0.8308 - val_loss: 0.6825\n",
      "Epoch 11/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9538 - loss: 0.2156 - val_accuracy: 0.8231 - val_loss: 0.6375\n",
      "Epoch 12/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9366 - loss: 0.2221 - val_accuracy: 0.8308 - val_loss: 0.6208\n",
      "Epoch 13/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9211 - loss: 0.2476 - val_accuracy: 0.8462 - val_loss: 0.6624\n",
      "Epoch 14/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9571 - loss: 0.1489 - val_accuracy: 0.8385 - val_loss: 0.7308\n",
      "Epoch 15/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9711 - loss: 0.1177 - val_accuracy: 0.8462 - val_loss: 0.6318\n",
      "Epoch 16/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9587 - loss: 0.1523 - val_accuracy: 0.8308 - val_loss: 0.7035\n",
      "Epoch 17/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9648 - loss: 0.1136 - val_accuracy: 0.8308 - val_loss: 0.6887\n",
      "Epoch 18/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9835 - loss: 0.0914 - val_accuracy: 0.8308 - val_loss: 0.7414\n",
      "Epoch 19/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9718 - loss: 0.0816 - val_accuracy: 0.8462 - val_loss: 0.7367\n",
      "Epoch 20/20\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9888 - loss: 0.0705 - val_accuracy: 0.8462 - val_loss: 0.7282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2111b19b760>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df59433-867d-489a-ae5f-012eca7ffdf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8df831ec-d6b1-464c-a214-5809d54a87a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete and saved!\n"
     ]
    }
   ],
   "source": [
    "# Save Model\n",
    "model.save(\"audio_classification_model.h5\")\n",
    "print(\"Model training complete and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "871a0c49-812d-44ea-b10b-c0aceb40ad8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_encoder.pkl']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(le, \"label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "29c95799-3bce-44eb-abc9-cfc7ae8a8a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "\n",
    "# Load assets\n",
    "model = load_model(\"audio_classification_model.h5\")\n",
    "le = joblib.load(\"label_encoder.pkl\")\n",
    "yamnet = hub.load(\"https://tfhub.dev/google/yamnet/1\")\n",
    "\n",
    "def predict_audio_class(audio_path):\n",
    "    try:\n",
    "        # Extract embedding\n",
    "        y, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
    "        waveform = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "        scores, embeddings, spectrogram = yamnet(waveform)\n",
    "        mean_embedding = np.mean(embeddings.numpy()[0], axis=0)\n",
    "        \n",
    "        # Predict\n",
    "        embedding = np.expand_dims(mean_embedding, axis=0)\n",
    "        predictions = model.predict(embedding)\n",
    "        predicted_idx = np.argmax(predictions)\n",
    "        return le.inverse_transform([predicted_idx])[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Prediction error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b0fabdf9-9700-4797-9905-02e4334389fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_audio_class(audio_path, model, label_encoder):\n",
    "    # Extract features using YAMNet\n",
    "    embedding = extract_embedding(audio_path)\n",
    "    \n",
    "    # Verify embedding shape\n",
    "    print(\"Raw embedding shape:\", embedding.shape)  # Should be (1024,)\n",
    "    \n",
    "    # Reshape to (1, 1024) for batch inference\n",
    "    embedding = np.expand_dims(embedding, axis=0)\n",
    "    print(\"Reshaped embedding shape:\", embedding.shape)  # Should be (1, 1024)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict(embedding)\n",
    "    predicted_class_idx = np.argmax(predictions)\n",
    "    predicted_label = label_encoder.inverse_transform([predicted_class_idx])[0]\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5f731ba7-ee86-4a3a-931f-11f23550ad71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw embedding shape: (1024,)\n",
      "Reshaped embedding shape: (1, 1024)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Horse\n"
     ]
    }
   ],
   "source": [
    "print(predict_audio_class(\"C:/Users/5A_Traders/Downloads/FYP_ON_DEV/FYP_IntelliTrain/AudioClassification/Dataset/archive/DataTest/horse/horse3.wav\",model,le)) # Example: returns \"cat\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21899cc9-b6a1-49d7-bac9-9ab082931e50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
