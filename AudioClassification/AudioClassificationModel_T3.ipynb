{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1063507c-1054-4971-804e-0a55a8d2513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Bidirectional, LSTM, Reshape\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import os\n",
    "import audiomentations as A  ## NEW ##\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "253d49ac-cb04-4198-8c02-e8b2ecbf3123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Constants ---\n",
    "YAMNET_HANDLE = \"https://tfhub.dev/google/yamnet/1\"\n",
    "AUDIO_DIR = \"C:/Users/5A_Traders/Downloads/FYP_ON_DEV/FYP_IntelliTrain/AudioClassification/Dataset/animal_audio/Animal-Soundprepros\"  # Update path\n",
    "SAMPLE_RATE = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f313f163-c0a9-4396-87dc-f71615493ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Augmentation Setup ---  ## NEW ##\n",
    "augment = A.Compose([\n",
    "    A.AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "    A.TimeStretch(min_rate=0.8, max_rate=1.2, p=0.5),\n",
    "    A.PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "    A.TimeMask(min_band_part=0.2, max_band_part=0.5) \n",
    "      \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e3d8d8bc-f290-4dfb-ae3c-4a9529a5c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Extraction with Augmentation ---\n",
    "def extract_embedding(audio_file, apply_augmentation=True):  # Changed parameter name\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_file, sr=SAMPLE_RATE, mono=True)\n",
    "        \n",
    "        if apply_augmentation:  # Now using correct reference\n",
    "            y = augment(samples=y, sample_rate=sr)  # Now refers to audiomentations object\n",
    "            \n",
    "        waveform = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "        scores, embeddings, _ = yamnet(waveform)\n",
    "        \n",
    "        if len(embeddings.shape) == 2:\n",
    "            embeddings = tf.expand_dims(embeddings, 0)\n",
    "            \n",
    "        return np.mean(embeddings.numpy()[0], axis=0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_file}: {e}\")\n",
    "        return np.zeros(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e5e11cb2-cad2-4f2a-b3ac-9867c022357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Enhanced Model Architecture ---  ## NEW ##\n",
    "def create_model(num_classes):\n",
    "    model = Sequential([\n",
    "        Input(shape=(1024,)),\n",
    "        Reshape((1, 1024)),  # Prepare for sequence processing\n",
    "        Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.001))),\n",
    "        Bidirectional(LSTM(64, return_sequences=False)),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(0.01)),  # FC layer\n",
    "        Dropout(0.4),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Learning Rate Schedule  ## NEW ##\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate=1e-3,\n",
    "        decay_steps=1000,\n",
    "        decay_rate=0.9\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "eb834436-ef7e-4206-a45c-e855ca7e3741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading ---\n",
    "def load_data(directory):\n",
    "    features, labels = [], []\n",
    "    for class_label in os.listdir(directory):\n",
    "        class_path = os.path.join(directory, class_label)\n",
    "        if os.path.isdir(class_path):\n",
    "            for audio_file in os.listdir(class_path):\n",
    "                file_path = os.path.join(class_path, audio_file)\n",
    "                try:\n",
    "                    # Use augmentation for training data  ## NEW ##\n",
    "                    feature = extract_embedding(file_path, apply_augmentation=True)\n",
    "                    features.append(feature)\n",
    "                    labels.append(class_label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "    return np.array(features), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f16001-7ee6-432c-ba58-4bca5790aca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Main Execution ---\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Load YAMNet\n",
    "#     yamnet = hub.load(YAMNET_HANDLE)\n",
    "    \n",
    "#     # Load and preprocess data\n",
    "#     X, y = load_data(AUDIO_DIR)\n",
    "#     le = LabelEncoder()\n",
    "#     y_encoded = le.fit_transform(y)\n",
    "    \n",
    "#     # Split data\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "#     )\n",
    "    \n",
    "#     # Handle class imbalance  ## NEW ##\n",
    "#     class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "#     class_weights = dict(enumerate(class_weights))\n",
    "    \n",
    "#     # Create and train model\n",
    "#     model = create_model(len(le.classes_))\n",
    "#     history = model.fit(\n",
    "#         X_train, y_train,\n",
    "#         validation_data=(X_test, y_test),\n",
    "#         epochs=50,\n",
    "#         batch_size=32,\n",
    "#         class_weight=class_weights  ## NEW ##\n",
    "#     )\n",
    "    \n",
    "#     # Save artifacts\n",
    "#     model.save(\"enhanced_audio_model.h5\")\n",
    "#     joblib.dump(le, \"label_encoder.pkl\")\n",
    "#     print(\"Model training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "54708334-c06f-46a2-85f8-1f3be15dd0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "06e52911-c227-4360-afe6-f2281cd1c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Load YAMNet\n",
    "yamnet = hub.load(YAMNET_HANDLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3e126797-e7ca-4d75-884f-dab0df193b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Load and preprocess data\n",
    "X, y = load_data(AUDIO_DIR)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a3e22192-2cc4-47f4-81c3-fff9a5c5d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8403f155-bdd0-41e8-96e8-40c0c6bcafeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle class imbalance  ## NEW ##\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9682d191-d193-4489-bb2c-13985557207a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 13\n",
      "Unique labels in y_train: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of classes:\", len(le.classes_))\n",
    "print(\"Unique labels in y_train:\", np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "13b5dcc6-5122-4ee7-88f8-92ab409b0f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (520, 1024)\n",
      "y_train shape: (520,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)  # Should be (samples, 1024)\n",
    "print(\"y_train shape:\", y_train.shape)  # Should be (samples,)\n",
    "assert X_train.shape[1] == 1024\n",
    "assert len(y_train.shape) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8e2bfb5e-9ab9-484b-9fdd-fae5f2bba42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=5,  # Stop if no improvement for 5 epochs\n",
    "        restore_best_weights=True,\n",
    "        verbose=1 \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e7750227-26a7-44da-85cc-37880e57e3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 136ms/step - accuracy: 0.1553 - loss: 5.2864 - val_accuracy: 0.4077 - val_loss: 4.3182\n",
      "Epoch 2/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.3548 - loss: 4.0950 - val_accuracy: 0.4692 - val_loss: 3.4380\n",
      "Epoch 3/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.3737 - loss: 3.2892 - val_accuracy: 0.4769 - val_loss: 2.6803\n",
      "Epoch 4/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.4295 - loss: 2.5682 - val_accuracy: 0.5538 - val_loss: 2.1008\n",
      "Epoch 5/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5751 - loss: 2.0464 - val_accuracy: 0.6077 - val_loss: 1.8553\n",
      "Epoch 6/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.6002 - loss: 1.8208 - val_accuracy: 0.5846 - val_loss: 1.6796\n",
      "Epoch 7/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.6592 - loss: 1.6182 - val_accuracy: 0.6615 - val_loss: 1.6014\n",
      "Epoch 8/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.7200 - loss: 1.3897 - val_accuracy: 0.6385 - val_loss: 1.5495\n",
      "Epoch 9/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.7204 - loss: 1.3957 - val_accuracy: 0.7077 - val_loss: 1.5426\n",
      "Epoch 10/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.7665 - loss: 1.2476 - val_accuracy: 0.7000 - val_loss: 1.5313\n",
      "Epoch 11/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.7668 - loss: 1.2364 - val_accuracy: 0.6385 - val_loss: 1.5595\n",
      "Epoch 12/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.7762 - loss: 1.1406 - val_accuracy: 0.6538 - val_loss: 1.5309\n",
      "Epoch 13/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.7591 - loss: 1.1334 - val_accuracy: 0.6846 - val_loss: 1.4599\n",
      "Epoch 14/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.8007 - loss: 1.0658 - val_accuracy: 0.7154 - val_loss: 1.4002\n",
      "Epoch 15/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.8246 - loss: 0.9805 - val_accuracy: 0.7154 - val_loss: 1.3993\n",
      "Epoch 16/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.8471 - loss: 0.9419 - val_accuracy: 0.7077 - val_loss: 1.4488\n",
      "Epoch 17/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.8462 - loss: 0.9607 - val_accuracy: 0.6692 - val_loss: 1.4579\n",
      "Epoch 18/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.8620 - loss: 0.8982 - val_accuracy: 0.6692 - val_loss: 1.4271\n",
      "Epoch 19/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.8904 - loss: 0.8173 - val_accuracy: 0.6692 - val_loss: 1.5181\n",
      "Epoch 19: early stopping\n",
      "Restoring model weights from the end of the best epoch: 14.\n"
     ]
    }
   ],
   "source": [
    "# Create and train model\n",
    "model = create_model(len(le.classes_))\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weights,\n",
    "    callbacks = [early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3dd828f1-2aad-4181-808d-ee1e0daaf36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete!\n"
     ]
    }
   ],
   "source": [
    "# Save artifacts\n",
    "model.save(\"enhanced_audio_model.h5\")\n",
    "joblib.dump(le, \"label_encoder.pkl\")\n",
    "print(\"Model training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c80c4832-9c72-4b2a-8f9a-ee5d721906e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Enhanced Prediction Function ---  ## NEW ##\n",
    "def predict_audio_class(audio_path, model, le):\n",
    "    try:\n",
    "        # Extract features without augmentation for prediction\n",
    "        embedding = extract_embedding(audio_path, apply_augmentation=False)\n",
    "        embedding = np.expand_dims(embedding, axis=0)\n",
    "        \n",
    "        # Get prediction probabilities\n",
    "        probs = model.predict(embedding)[0]\n",
    "        pred_idx = np.argmax(probs)\n",
    "        confidence = probs[pred_idx]\n",
    "        \n",
    "        return le.inverse_transform([pred_idx])[0], confidence\n",
    "    except Exception as e:\n",
    "        print(f\"Prediction error: {e}\")\n",
    "        return None, 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5f731ba7-ee86-4a3a-931f-11f23550ad71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "('Bear', 0.5269819)\n"
     ]
    }
   ],
   "source": [
    "print(predict_audio_class(\"C:/Users/5A_Traders/Downloads/FYP_ON_DEV/FYP_IntelliTrain/AudioClassification/Dataset/archive/DataTest/lion/lion1.wav\",model,le)) # Example: returns \"cat\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21899cc9-b6a1-49d7-bac9-9ab082931e50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
